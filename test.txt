import time
import random
import json
from abc import ABC, abstractmethod
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Any, Sequence
from datetime import datetime

import numpy as np
import torch
import matplotlib.pyplot as plt
from neuralop.data.datasets import load_darcy_flow_small

###############################################################################
# 1.  Pure‑PyTorch DeepONet implementation                                  #
###############################################################################

class DeepONet(torch.nn.Module):
    """A minimal DeepONet suitable for 16×16 Darcy flow."""

    def __init__(self, branch_input_size: int, trunk_input_size: int = 2,
                 hidden_size: int = 128, num_layers: int = 4):
        super().__init__()
        self.branch_input_size = branch_input_size
        self.trunk_input_size = trunk_input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # ----- branch -----
        branch = [torch.nn.Linear(branch_input_size, hidden_size), torch.nn.ReLU()]
        for _ in range(num_layers - 1):
            branch.extend([torch.nn.Linear(hidden_size, hidden_size), torch.nn.ReLU()])
        branch.append(torch.nn.Linear(hidden_size, hidden_size))  # no activation
        self.branch_net = torch.nn.Sequential(*branch)
        # ----- trunk -----
        trunk = [torch.nn.Linear(trunk_input_size, hidden_size), torch.nn.ReLU()]
        for _ in range(num_layers - 1):
            trunk.extend([torch.nn.Linear(hidden_size, hidden_size), torch.nn.ReLU()])
        trunk.append(torch.nn.Linear(hidden_size, hidden_size))
        self.trunk_net = torch.nn.Sequential(*trunk)
        # bias term
        self.bias = torch.nn.Parameter(torch.zeros(1))

    def forward(self, branch_in: torch.Tensor, trunk_in: torch.Tensor) -> torch.Tensor:
        """branch_in: [B, n_sensors]  trunk_in: [N, 2] returns [B, N]"""
        b = self.branch_net(branch_in)          # [B, H]
        t = self.trunk_net(trunk_in)            # [N, H]
        out = torch.sum(b.unsqueeze(1) * t.unsqueeze(0), dim=2)  # tensor product
        return out + self.bias

###############################################################################
# 2.  Metrics                                                                #
###############################################################################

def mse(p, y):
    return torch.mean((p - y) ** 2).item()

def mae(p, y):
    return torch.mean(torch.abs(p - y)).item()

def rel_l2(p, y):
    return (torch.linalg.vector_norm(p - y) / torch.linalg.vector_norm(y)).item()

METRICS = dict(mse=mse, mae=mae, rel_l2=rel_l2)

###############################################################################
# 3.  Operator interface & adapters                                          #
###############################################################################

class BaseOperator(ABC):
    """Every model wrapper must implement this interface."""

    def __init__(self, device: torch.device):
        self.device = device

    @abstractmethod
    def setup(self, data_info: Dict[str, Any]):
        ...

    @abstractmethod
    def train_epoch(self, loader: torch.utils.data.DataLoader) -> float:
        ...

    @abstractmethod
    def predict(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        ...

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """Return model architecture information"""
        ...

    # ----------------------------------------------------
    def eval(self, loader, metrics: Dict[str, callable]):
        self.model.eval()
        agg = {k: 0.0 for k in metrics}
        with torch.no_grad():
            for batch in loader:
                pred = self.predict(batch)
                true = batch["y"].to(self.device)
                for k, f in metrics.items():
                    agg[k] += f(pred, true)
        return {k: v / len(loader) for k, v in agg.items()}

    def count_parameters(self):
        """Count trainable parameters"""
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)


class FNOOperator(BaseOperator):
    """Thin adapter around Neural Operator FNO."""

    def setup(self, data_info):
        from neuralop.models import FNO

        self.n_modes = (16, 16)
        self.hidden_channels = 32
        self.n_layers = 3
        
        self.model = FNO(
            n_modes=self.n_modes,
            hidden_channels=self.hidden_channels,
            in_channels=1,
            out_channels=1,
            lifting_channels=32,
            projection_channels=32,
            n_layers=self.n_layers,
        ).to(self.device)
        self.opt = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        self.loss = torch.nn.MSELoss()

    def train_epoch(self, loader):
        self.model.train()
        running = 0.0
        for batch in loader:
            self.opt.zero_grad()
            pred = self.model(batch["x"].to(self.device))
            loss = self.loss(pred, batch["y"].to(self.device))
            loss.backward()
            self.opt.step()
            running += loss.item()
        return running / len(loader)

    def predict(self, batch):
        return self.model(batch["x"].to(self.device))

    def get_model_info(self):
        return {
            "name": "Fourier Neural Operator (FNO)",
            "architecture": {
                "type": "Spectral convolution in Fourier space",
                "n_modes": self.n_modes,
                "hidden_channels": self.hidden_channels,
                "n_layers": self.n_layers,
                "lifting_channels": 32,
                "projection_channels": 32
            },
            "parameters": self.count_parameters(),
            "optimizer": "Adam",
            "learning_rate": 1e-3
        }


class DeepONetOperator(BaseOperator):
    """Adapter for our PyTorch DeepONet."""

    def __init__(self, device, n_sensors=100):
        super().__init__(device)
        self.n_sensors = n_sensors

    def setup(self, data_info):
        self.sensor_idx = data_info["sensor_idx"]
        self.trunk = torch.FloatTensor(data_info["coords"]).to(self.device)
        in_branch = len(self.sensor_idx)
        self.model = DeepONet(in_branch, trunk_input_size=2, hidden_size=128).to(
            self.device
        )
        self.opt = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        self.loss = torch.nn.MSELoss()
        # Store sensor indices as list for JSON serialization
        self.sensor_idx_list = self.sensor_idx.tolist() if hasattr(self.sensor_idx, 'tolist') else list(self.sensor_idx)

    # helpers -------------------------------------------
    def _take_sensors(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B, 1, H, W] -> [B, H*W] -> select
        B = x.shape[0]
        flat = x.view(B, -1)
        return flat[:, self.sensor_idx]

    # main interface ------------------------------------
    def train_epoch(self, loader):
        self.model.train()
        run = 0.0
        for batch in loader:
            self.opt.zero_grad()
            branch = self._take_sensors(batch["x"].to(self.device))
            pred = self.model(branch, self.trunk)
            tgt = batch["y"].to(self.device).view(pred.shape)
            loss = self.loss(pred, tgt)
            loss.backward()
            self.opt.step()
            run += loss.item()
        return run / len(loader)

    def predict(self, batch):
        branch = self._take_sensors(batch["x"].to(self.device))
        pred = self.model(branch, self.trunk)
        # reshape back to [B,1,H,W]
        B = branch.shape[0]
        H = int(np.sqrt(pred.shape[1]))
        return pred.view(B, 1, H, H)

    def get_model_info(self):
        return {
            "name": "Deep Operator Network (DeepONet)",
            "architecture": {
                "type": "Branch-Trunk neural network",
                "branch_input_size": self.model.branch_input_size,
                "trunk_input_size": self.model.trunk_input_size,
                "hidden_size": self.model.hidden_size,
                "num_layers": self.model.num_layers,
                "n_sensors": self.n_sensors
            },
            "parameters": self.count_parameters(),
            "optimizer": "Adam",
            "learning_rate": 1e-3
        }

###############################################################################
# 4.  Data module                                                            #
###############################################################################

@dataclass
class DataModule:
    grid: int = 16
    n_train: int = 100
    batch: int = 4
    n_test: int = 50

    def setup(self):
        tr_loader, te_loaders, _ = load_darcy_flow_small(
            n_train=self.n_train,
            batch_size=self.batch,
            test_resolutions=[self.grid],
            n_tests=[self.n_test],
            test_batch_sizes=[self.batch],
        )
        self.train = tr_loader
        self.test = te_loaders[self.grid]
        # sensor subset
        rng = np.random.default_rng(42)
        N = self.grid ** 2
        sensor_idx = rng.choice(N, size=100, replace=False)
        x = np.linspace(0, 1, self.grid)
        X, Y = np.meshgrid(x, x)
        coords = np.column_stack([X.flatten(), Y.flatten()])
        self.info = dict(sensor_idx=sensor_idx, coords=coords)

    def get_data_info(self):
        """Return dataset characteristics"""
        return {
            "name": "Darcy Flow",
            "description": "2D steady-state Darcy flow equation with variable permeability",
            "equation": "∇ · (κ(x)∇p(x)) = f(x)",
            "input": "Permeability field κ(x)",
            "output": "Pressure field p(x)",
            "grid_resolution": f"{self.grid}×{self.grid}",
            "n_train_samples": self.n_train,
            "n_test_samples": self.n_test,
            "batch_size": self.batch,
            "spatial_domain": "[0, 1]²",
            "n_grid_points": self.grid ** 2
        }

# 5.  Visualizer                                                             #

class Visualizer:
    def __init__(self, dm: DataModule, out_dir="figs"):
        self.dm = dm
        self.out = Path(out_dir)
        self.out.mkdir(exist_ok=True)

    @torch.no_grad()
    def sample_and_plot(self, operator: BaseOperator, n: int = 2, fname: str | None = None):
        batch = next(iter(self.dm.test))
        preds = operator.predict(batch).cpu()
        x = batch["x"].cpu().numpy()
        y = batch["y"].cpu().numpy()
        p = preds.numpy()
        
        errors = np.abs(p - y)
        
        fig, ax = plt.subplots(n, 4, figsize=(14, 4 * n))
        for i in range(n):
            im0 = ax[i, 0].imshow(x[i, 0], cmap="viridis")
            ax[i, 0].set_title("Input κ(x)")
            ax[i, 0].axis("off")
            plt.colorbar(im0, ax=ax[i, 0], fraction=0.046)
            
            im1 = ax[i, 1].imshow(y[i, 0], cmap="viridis")
            ax[i, 1].set_title("True p(x)")
            ax[i, 1].axis("off")
            plt.colorbar(im1, ax=ax[i, 1], fraction=0.046)
            
            im2 = ax[i, 2].imshow(p[i, 0], cmap="viridis")
            ax[i, 2].set_title("Predicted p̂(x)")
            ax[i, 2].axis("off")
            plt.colorbar(im2, ax=ax[i, 2], fraction=0.046)
            
            im3 = ax[i, 3].imshow(errors[i, 0], cmap="hot")
            ax[i, 3].set_title(f"Absolute Error (max: {errors[i, 0].max():.3f})")
            ax[i, 3].axis("off")
            plt.colorbar(im3, ax=ax[i, 3], fraction=0.046)
            
        fig.tight_layout()
        fpath = self.out / (fname or f"{operator.__class__.__name__}.png")
        fig.savefig(fpath, dpi=150, bbox_inches='tight')
        plt.close(fig)
        print(f"[Visualizer] saved {fpath}")
        return str(fpath)

# 6.  Benchmark runner                                                       #

class BenchmarkRunner:
    def __init__(self, models: Sequence[BaseOperator], dm: DataModule, epochs: int = 25):
        self.models = models
        self.dm = dm
        self.epochs = epochs
        self.vis = Visualizer(dm)
        self.train_history = {}

    def run(self):
        results = []
        
        for m in self.models:
            model_name = m.__class__.__name__
            print(f"\n===== {model_name} =====")
            m.setup(self.dm.info)
            
            train_losses = []
            
            t0 = time.time()
            for ep in range(1, self.epochs + 1):
                tr_loss = m.train_epoch(self.dm.train)
                train_losses.append(tr_loss)
                if ep % 5 == 0:
                    print(f"  epoch {ep:02d}/{self.epochs}  train_loss={tr_loss:.4e}")
            wall = time.time() - t0
            
            self.train_history[model_name] = train_losses
            
            metrics = m.eval(self.dm.test, METRICS)
            metrics["wall_sec"] = wall
            metrics["avg_time_per_epoch"] = wall / self.epochs
            
            model_info = m.get_model_info()
            
            plot_path = self.vis.sample_and_plot(m)
            
            results.append({
                "name": model_name,
                "model_info": model_info,
                "metrics": metrics,
                "plot_path": plot_path
            })
            
        return results

    def save_results(self, results, output_path="benchmark_results.json"):
        """Save all results to JSON"""
        clean_results = []
        for r in results:
            clean_model_info = r["model_info"].copy()
            
            if "architecture" in clean_model_info:
                arch = clean_model_info["architecture"]
                for k, v in arch.items():
                    if hasattr(v, 'tolist'):
                        arch[k] = v.tolist()
                    elif isinstance(v, (list, tuple)) and len(v) > 0 and hasattr(v[0], 'item'):
                        arch[k] = [x.item() if hasattr(x, 'item') else x for x in v]
            
            clean_results.append({
                "name": r["name"],
                "model_info": clean_model_info,
                "metrics": {k: float(v) for k, v in r["metrics"].items()},
                "plot_path": str(r["plot_path"])
            })
        
        output = {
            "timestamp": datetime.now().isoformat(),
            "device": str(self.device) if hasattr(self, 'device') else "unknown",
            "epochs": self.epochs,
            "dataset_info": self.dm.get_data_info(),
            "models": clean_results,
            "training_history": {
                model: [float(loss) for loss in losses] 
                for model, losses in self.train_history.items()
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(output, f, indent=2)
        
        print(f"\n[BenchmarkRunner] Results saved to {output_path}")

# 7.  Main                                                                   #

if __name__ == "__main__":
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    print(f"Using device: {device}")

    dm = DataModule()
    dm.setup()
    
    models = [FNOOperator(device), DeepONetOperator(device)]
    runner = BenchmarkRunner(models, dm, epochs=100)
    runner.device = device  # Store device info
    
    scores = runner.run()

    print("\n===== RESULTS =====")
    for s in scores:
        print(f"\n{s['name']}:")
        print(f"  Parameters: {s['model_info']['parameters']:,}")
        print(f"  Metrics: {s['metrics']}")
    
    runner.save_results(scores)